{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "American_Options_lower_bound_Change_NN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqH_FvLm8zpR",
        "colab_type": "code",
        "outputId": "5681a748-2b51-433a-c35f-ee9c2093c19e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "#import simulation as sl\n",
        "import time as t\n",
        "tf.config.experimental.list_physical_devices('GPU')"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01QH5Aw99JCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multiBrownian(M, N, dim, T):\n",
        "    '''\n",
        "    A multidimensional independent Brownian motion.\n",
        "    M: Number of samples.\n",
        "    N: Number of periods.\n",
        "    dim: Dimension of the brownian motion.\n",
        "    T: Time interval\n",
        "    '''\n",
        "    \n",
        "    dt = tf.convert_to_tensor(T / (N-1), dtype=tf.float64)\n",
        "    Z = tf.math.sqrt(dt) * tf.random.normal((M, N, dim), dtype=tf.float64)\n",
        "    return tf.math.cumsum(Z, axis=1)\n",
        "\n",
        "def geometricBM(nb_samples, nb_periods, dim, T, S0, rate, div_yield, sigma, corr):\n",
        "    '''\n",
        "    This function will simulate a geometric BM\n",
        "    \n",
        "    S0: Initial value. shape = (dim)\n",
        "    rate: Risk free interest rate (scalar).\n",
        "    div_yield: Dividends yields. shape = (dim)\n",
        "    sigma: Volatilities. shape = (dim)\n",
        "    corr: Correlation matrix. shape = (dim, dim) \n",
        "    '''\n",
        "    # convert to tensor\n",
        "    S0 = tf.convert_to_tensor(S0, dtype=tf.float64)\n",
        "    div_yield = tf.convert_to_tensor(div_yield, dtype=tf.float64)\n",
        "    sigma = tf.convert_to_tensor(sigma, dtype=tf.float64)\n",
        "    corr = tf.convert_to_tensor(corr, dtype=tf.float64)\n",
        "        \n",
        "    # time grid\n",
        "    t = tf.range(0, T + T / nb_periods, T / (nb_periods - 1), dtype=tf.float64) \n",
        "    t = tf.reshape(t, [nb_periods, 1])\n",
        "    \n",
        "    # drift\n",
        "    u = rate - div_yield - sigma ** 2 / 2\n",
        "    u = tf.reshape(u, [1, dim])\n",
        "\n",
        "    # get brownian motion\n",
        "    BM = multiBrownian(nb_samples, nb_periods, dim, T)    \n",
        "    \n",
        "    if dim > 1:\n",
        "        # covariance matrix -------------------\n",
        "        #temp = sigma[None] * sigma[:, None]\n",
        "        #cov = tf.multiply(temp, corr)\n",
        "        #A = tf.linalg.cholesky(cov)\n",
        "\n",
        "        # or\n",
        "        sigma_ = tf.reshape(sigma, [dim, 1])\n",
        "        A = tf.linalg.cholesky(corr)\n",
        "        A = tf.multiply(A, sigma_)\n",
        "        # -------------------------------------        \n",
        "        diffusion_term = tf.linalg.matvec(A, BM)  \n",
        "    else:\n",
        "        diffusion_term = sigma*BM\n",
        "    \n",
        "    res = tf.math.exp(u*t  + diffusion_term)    \n",
        "    return S0 * res\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOoTdBIS_F7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def payoff (tau, x, var_compute = False):\n",
        "    '''\n",
        "    The payoff of the option\n",
        "    Tau will be a vector one of the positions in the array of x\n",
        "    '''\n",
        "    P = tf.math.reduce_max(x, axis = 1) - K\n",
        "    t2 = tf.cast(tau, float_type)        \n",
        "    \n",
        "    I = tf.math.greater(P,0)\n",
        "    I = tf.where(I, 1.0, 1.0*0)    \n",
        "    I = tf.cast(I, float_type)\n",
        "    pay = tf.convert_to_tensor(tf.exp(-r * t2*T/n), dtype = float_type)*tf.multiply(I,P)\n",
        "    if var_compute:\n",
        "        return pay\n",
        "    else:\n",
        "        return tf.reduce_mean(pay)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2wbUL_njwAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import stats\n",
        "import numpy as np\n",
        "def confident_interval(sigma, K, alpha = 0.05):\n",
        "    '''\n",
        "    This function will return\n",
        "    z_(alpha/2) * sigma/sqrt(K)\n",
        "    '''\n",
        "    return stats.norm.ppf(1-alpha/2)*sigma/np.sqrt(K)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qstZZWrQ_V5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Pricer:\n",
        "    '''\n",
        "    This class will initializate each child with a diferent Neural Network\n",
        "    This way it can change each child's loss function\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, n):\n",
        "        self.NN = [NeuralNet(i + 1) for i in range(n)]\n",
        "        self.n = n\n",
        "        self.this_stop_time = self.n * tf.ones(batch_size*training_size, dtype = int_type)\n",
        "        \n",
        "    def update_stop_time (self, n, X):\n",
        "        f = tf.squeeze(self.NN[n].run(X, batch_size, training_size))\n",
        "        f = tf.cast(f, dtype = int_type)\n",
        "        self.this_stop_time = n * f + self.this_stop_time * (1 - f)\n",
        "    \n",
        "    def train(self):\n",
        "        print('Generating simulated samples X')\n",
        "        start = t.time()\n",
        "        X = geometricBM(batch_size * training_size, n+1, d, T, S0, r, delta, sigma, pho)\n",
        "        \n",
        "        input_nn = tf.Variable(X, name = 'x', dtype = float_type)\n",
        "        for i in range(self.n - 1, -1, -1):\n",
        "            print('Training layer {} ...'.format(i))\n",
        "            self.NN[i].network_learn(input_nn, self.this_stop_time)\n",
        "            \n",
        "            self.update_stop_time(i, input_nn)\n",
        "        print('End of training - Elapsed time = {}s'.format(t.time() - start))\n",
        "\n",
        "    def simulate(self, i, Z):\n",
        "        '''\n",
        "        Z has shape: (J, n - i + 1, )\n",
        "        '''\n",
        "\n",
        "\n",
        "    def lower_bound(self, Kl, t_size):\n",
        "        '''\n",
        "        Return a lower bound estimate together with the variance for this estimate: (lower_bound, variance).\n",
        "        The actual sample size is Kl * t_size. \n",
        "        Use t_size to divide the simulation in t_size batches of size Kl. This is preferred over a single batch of size Kl * t_size if do not have enough ram. \n",
        "        '''\n",
        "\n",
        "        X = geometricBM(Kl * t_size, n+1, d, T, S0, r, delta, sigma, pho)\n",
        "            \n",
        "        input_nn = tf.Variable(X, name = 'x', dtype = float_type)\n",
        "        s = []\n",
        "        for i in range(self.n):\n",
        "            s.append( self.NN[i].run(input_nn, Kl, t_size))\n",
        "        s.append(tf.ones([Kl * t_size], dtype = int_type)) \n",
        "        \n",
        "        tau = 0\n",
        "        for i in range(self.n + 1):\n",
        "            p = 1\n",
        "            for j in range(i):\n",
        "                p *= 1 - s[j]\n",
        "            \n",
        "            tau += i * s[i] * p\n",
        "        \n",
        "        tau = tf.squeeze(tau)\n",
        "        t1 = tf.range(input_nn.shape[0], dtype = int_type)\n",
        "        Indx = tf.stack((t1, tau), axis=1)\n",
        "        X_stoped = tf.gather_nd(input_nn,Indx)\n",
        "        print(tf.math.reduce_variance(tau))\n",
        "\n",
        "        L = payoff (tf.squeeze(tau), X_stoped, var_compute = True)\n",
        "        \n",
        "        L_mean = tf.reduce_mean(L)\n",
        "        \n",
        "        variance = (L - L_mean)**2\n",
        "        variance = tf.sqrt(tf.math.reduce_sum(variance)/(Kl * t_size - 1))\n",
        "        \n",
        "        return L_mean, variance\n",
        "    '''\n",
        "    def upper_bound(self, Ku, J):\n",
        "        \n",
        "        Return an upper bound estimate together with the variance for this estimate: (upper_bound, variance).\n",
        "        The actual sample size is Kl * t_size. \n",
        "        Use t_size to divide the simulation in t_size batches of size Kl. This is preferred over a single batch of size Kl * t_size if do not have enough ram. \n",
        "        \n",
        "\n",
        "        Z = geometricBM(Ku, n + 1, d, T, S0, r, delta, sigma, pho)\n",
        "\n",
        "        # get payoff for all paths and time periods\n",
        "        # change this - implement this as a function call to payoff function\n",
        "        time_range = tf.range(0, n+1, dtype=float_type)\n",
        "        G = tf.math.reduce_max(Z, axis = 2) - K\n",
        "        I = tf.cast(tf.where(G > 0, 1.0, 0.0), float_type)        \n",
        "        G = tf.exp(-r * time_range * T/n) * tf.multiply(I, G)\n",
        "\n",
        "        for i in range(0, n):\n",
        "            C_j = tf.zeros((1, Ku))\n",
        "            for j in range(0, Ku):\n",
        "                # generate conditional paths ZZ\n",
        "                ZZ = geometricBM(J, n + 1 - i, dim, (n - i) * T / n, , r, delta, sigma, pho) # this includes the initial condition as t = 0 (i)\n",
        "    '''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI7oT5W8eFg-",
        "colab_type": "code",
        "outputId": "ebc07658-7dfa-4da6-b45f-fdf77395b8fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.cast(0.73, dtype=tf.int32)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int32, numpy=0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVQd58eu9ewV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNet:\n",
        "    '''\n",
        "    This class will be responsable for each individual neural network\n",
        "    In our model, it will be the F^(theta_n)\n",
        "    '''\n",
        "    def __init__(self, n):\n",
        "        xavier=tf.keras.initializers.GlorotUniform()\n",
        "        self.l1=tf.keras.layers.Dense(40 + d, kernel_initializer = xavier, activation=tf.nn.relu,input_shape=[d], dtype = float_type)\n",
        "        self.l2=tf.keras.layers.Dense(40 + d, kernel_initializer = xavier,activation=tf.nn.relu, dtype = float_type)\n",
        "        self.out=tf.keras.layers.Dense(1,kernel_initializer = xavier,activation = tf.nn.sigmoid, dtype = float_type)\n",
        "        self.train_op = tf.keras.optimizers.Adam(0.05)\n",
        "        self.n = n\n",
        "        self.stop_time = []\n",
        "        self.stop_X = []\n",
        "        \n",
        "        \n",
        "    # Running the model\n",
        "    def run(self, X, b_size, t_size): \n",
        "      ans = []\n",
        "      \n",
        "      for i in range(t_size):\n",
        "        X_test = X[i*b_size:(i + 1)*b_size, self.n]\n",
        "        boom=self.l1(X_test)\n",
        "        boom1=self.l2(boom)\n",
        "        boom2=self.out(boom1)\n",
        "        \n",
        "        boom2 = tf.where(tf.greater(boom2, 0.5), 1, 0)\n",
        "        ans.append(tf.cast(boom2, int_type)[:,0])\n",
        "      \n",
        "      return tf.concat(ans, axis = 0)\n",
        "      \n",
        "    #Custom loss fucntion\n",
        "    #Change this for each n\n",
        "    def get_loss(self, X):\n",
        "        boom=self.l1(X)\n",
        "        boom1=self.l2(boom)\n",
        "        boom2=self.out(boom1)\n",
        "        \n",
        "        r = payoff(self.n-1,X)*boom2 +  payoff(self.stop_time,self.stop_X)*(1-boom2)\n",
        "        return -r\n",
        "      \n",
        "    # get gradients\n",
        "    def get_grad(self,X):\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(self.l1.variables)\n",
        "            tape.watch(self.l2.variables)\n",
        "            tape.watch(self.out.variables)\n",
        "            L = self.get_loss(X)\n",
        "            g = tape.gradient(L, [self.l1.variables[0],self.l1.variables[1],self.l2.variables[0],self.l2.variables[1],self.out.variables[0],self.out.variables[1]])\n",
        "        return g\n",
        "      \n",
        "    # perform gradient descent\n",
        "    def network_learn(self,X, stop_time):   \n",
        "        t1 = tf.range(X.shape[0], dtype = int_type)\n",
        "        Indx = tf.stack((t1, stop_time), axis=1)\n",
        "        X_stoped = tf.gather_nd(X,Indx)\n",
        "        \n",
        "        for i in range(training_size):\n",
        "            self.stop_time = stop_time[i*batch_size:(i + 1)*batch_size]\n",
        "            self.stop_X = X_stoped[i * batch_size: (i + 1) * batch_size]\n",
        "            \n",
        "            g = self.get_grad(X[i * batch_size: (i + 1) * batch_size, self.n-1])\n",
        "            self.train_op.apply_gradients(\n",
        "                zip(g, [self.l1.variables[0],\n",
        "                        self.l1.variables[1],\n",
        "                        self.l2.variables[0],\n",
        "                        self.l2.variables[1],\n",
        "                        self.out.variables[0],\n",
        "                        self.out.variables[1]]))\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldKAnLJw_bsu",
        "colab_type": "code",
        "outputId": "12eecd6f-3412-4f60-e699-46f4caebf85a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "from IPython.display import clear_output\n",
        "float_type = tf.float64\n",
        "int_type = tf.int64\n",
        "\n",
        "n = 9\n",
        "d = 2\n",
        "T = 3\n",
        "S0 = 100*tf.ones(d, dtype=float_type)\n",
        "r = 0.05\n",
        "delta = 0.1*tf.ones(d, dtype=float_type)\n",
        "sigma = 0.2*tf.ones(d, dtype=float_type)\n",
        "pho = tf.linalg.diag(tf.ones(d, dtype = float_type))\n",
        "K = 100\n",
        "\n",
        "batch_size = 2**5\n",
        "training_size = 100\n",
        "P = Pricer(n) \n",
        "\n",
        "# train model\n",
        "P.train()\n",
        "\n",
        "# get lower bound estimate\n",
        "Kl = 10000\n",
        "L, var = P.lower_bound(Kl, 10)\n",
        "sig = confident_interval(var, Kl)\n",
        "\n",
        "print('\\nThe price is: {}'.format(L))  \n",
        "print(\"Variance is {}\".format(var))\n",
        "print(\"Confidence Interval: {}\".format(confident_interval(sig, Kl)))"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating simulated samples X\n",
            "Training layer 8 ...\n",
            "Training layer 7 ...\n",
            "Training layer 6 ...\n",
            "Training layer 5 ...\n",
            "Training layer 4 ...\n",
            "Training layer 3 ...\n",
            "Training layer 2 ...\n",
            "Training layer 1 ...\n",
            "Training layer 0 ...\n",
            "End of training - Elapsed time = 5.914351463317871s\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "\n",
            "The price is: 12.050735790624213\n",
            "Variance is 15.735680172892275\n",
            "Confidence Interval: 0.006044796739977869\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpZo5lyt732C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8jXtg-B736p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrVusJA274AE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt_wMB8673-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q8x3Srh735I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXbdks4-yNTI",
        "colab_type": "code",
        "outputId": "e0f140c8-fe8f-47d8-8f90-af65c90f99af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "for i in range(1):\n",
        "#  clear_output(wait = True)\n",
        " \n",
        "  \n",
        "  print(str(i) + ' The price is: ')\n",
        "  Kl = 100000\n",
        "  L, sig = P.lower_bound(Kl,100)\n",
        "  \n",
        "  print(\"Variance is = \")\n",
        "  print(sig)\n",
        "  print(\"Confidence Interval = \")\n",
        "  print(confident_interval(sig, Kl))"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 The price is: \n",
            "tf.Tensor(0, shape=(), dtype=int64)\n",
            "Variance is = \n",
            "tf.Tensor(15.763936421212463, shape=(), dtype=float64)\n",
            "Confidence Interval = \n",
            "tf.Tensor(0.09770409483432432, shape=(), dtype=float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh0awLu603xS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}