{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "American_Options_lower_bound.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqH_FvLm8zpR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "95484bd4-fdfd-4d92-ed43-223689265946"
      },
      "source": [
        "import tensorflow as tf\n",
        "import time as t\n",
        "tf.config.experimental.list_physical_devices('GPU')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01QH5Aw99JCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multiBrownian(M, N, dim, T):\n",
        "    '''\n",
        "    A multidimensional independent Brownian motion.\n",
        "    M: Number of samples.\n",
        "    N: Number of periods.\n",
        "    dim: Dimension of the brownian motion.\n",
        "    T: Time interval\n",
        "    '''\n",
        "    \n",
        "    dt = tf.convert_to_tensor(T / (N-1), dtype=tf.float64)\n",
        "    Z = tf.math.sqrt(dt) * tf.random.normal((M, N, dim), dtype=tf.float64)\n",
        "    return tf.math.cumsum(Z, axis=1, exclusive=True)\n",
        "\n",
        "def geometricBM(nb_samples, nb_periods, dim, T, S0, rate, div_yield, sigma, corr):\n",
        "    '''\n",
        "    This function will simulate a geometric BM\n",
        "    \n",
        "    S0: Initial value. shape = (dim)\n",
        "    rate: Risk free interest rate (scalar).\n",
        "    div_yield: Dividends yields. shape = (dim)\n",
        "    sigma: Volatilities. shape = (dim)\n",
        "    corr: Correlation matrix. shape = (dim, dim) \n",
        "    '''\n",
        "    # convert to tensor\n",
        "    S0 = tf.convert_to_tensor(S0, dtype=tf.float64)\n",
        "    div_yield = tf.convert_to_tensor(div_yield, dtype=tf.float64)\n",
        "    sigma = tf.convert_to_tensor(sigma, dtype=tf.float64)\n",
        "    corr = tf.convert_to_tensor(corr, dtype=tf.float64)\n",
        "        \n",
        "    # time grid\n",
        "    t = tf.range(0, T + T / nb_periods, T / (nb_periods - 1), dtype=tf.float64) \n",
        "    t = tf.reshape(t, [nb_periods, 1])\n",
        "    \n",
        "    # drift\n",
        "    u = rate - div_yield - sigma ** 2 / 2\n",
        "    u = tf.reshape(u, [1, dim])\n",
        "\n",
        "    # get brownian motion\n",
        "    BM = multiBrownian(nb_samples, nb_periods, dim, T)    \n",
        "    \n",
        "    if dim > 1:\n",
        "        # covariance matrix -------------------\n",
        "        #temp = sigma[None] * sigma[:, None]\n",
        "        #cov = tf.multiply(temp, corr)\n",
        "        #A = tf.linalg.cholesky(cov)\n",
        "\n",
        "        # or\n",
        "        sigma_ = tf.reshape(sigma, [dim, 1])\n",
        "        A = tf.linalg.cholesky(corr)\n",
        "        A = tf.multiply(A, sigma_)\n",
        "        # -------------------------------------        \n",
        "        diffusion_term = tf.linalg.matvec(A, BM)  \n",
        "    else:\n",
        "        diffusion_term = sigma*BM\n",
        "        \n",
        "    res = tf.math.exp(u*t + sigma**2/2*t + diffusion_term)    \n",
        "    return S0 * res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axKsPU43B1sj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "float_type = tf.float64\n",
        "int_type = tf.int64\n",
        "\n",
        "n = 9\n",
        "d = 2\n",
        "T = 3\n",
        "S0 = 110*tf.ones(d, dtype=float_type)\n",
        "r = 0.05\n",
        "delta = 0.1*tf.ones(d, dtype=float_type)\n",
        "sigma = 0.2*tf.ones(d, dtype=float_type)\n",
        "pho = tf.linalg.diag(tf.ones(d, dtype = float_type))\n",
        "K = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOoTdBIS_F7q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def payoff (tau, x, K = K, T = T, n=n, r=r):\n",
        "    '''\n",
        "    The payoff of the option\n",
        "    Tau will be a vector one of the positions in the array of x\n",
        "    '''\n",
        "    \n",
        "    P = tf.math.reduce_max(x, axis = 2) - K\n",
        "    \n",
        "    if type(tau) == int:\n",
        "         P = P[:,tau]   \n",
        "         \n",
        "    else: #In the event of receiving an array of stopping times\n",
        "        t1 = tf.range(P.shape[0], dtype = int_type)\n",
        "        Indx = tf.stack((t1, tau), axis=1)\n",
        "        P = tf.gather_nd(P,Indx)\n",
        "    \n",
        "    t2 = tf.cast(tau, float_type)        \n",
        "    \n",
        "    I = tf.math.greater(P,0)\n",
        "    I = tf.where(I, 1.0, 1.0*0)\n",
        "    \n",
        "    I = tf.cast(I, float_type)\n",
        "    \n",
        "    pay = tf.convert_to_tensor(tf.exp(-r * t2*T/n), dtype = float_type)*(I*P)\n",
        "    return tf.reduce_mean(pay)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qstZZWrQ_V5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Pricer:\n",
        "    '''\n",
        "    This class will initializate each child with a diferent Neural Network\n",
        "    This way it can change each child's loss function\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, n):\n",
        "        self.NN = [NeuralNet(i + 1) for i in range(n)]\n",
        "        self.n = n\n",
        "        self.this_stop_time = self.n*tf.ones(batch_size*training_size, dtype = int_type)\n",
        "        \n",
        "    def update_stop_time (self, n, X):\n",
        "        f = tf.squeeze(self.NN[n].run(X))\n",
        "        f = tf.cast(f, dtype = int_type)\n",
        "        self.this_stop_time = n * f + self.this_stop_time * (1 - f)\n",
        "    \n",
        "    def train(self):\n",
        "        print('Getting X')\n",
        "        start = t.time()\n",
        "        X = geometricBM(batch_size*training_size, n+1, d, T, S0, r, delta, sigma, pho)\n",
        "        input_nn = tf.Variable(X, name = 'x', dtype = float_type)\n",
        "        for i in range(self.n - 1, -1, -1):\n",
        "            print('Training layer i = ')\n",
        "            print(i)\n",
        "            self.NN[i].network_learn(input_nn, self.this_stop_time)\n",
        "            \n",
        "            self.update_stop_time(i, input_nn)\n",
        "        print('End of training, time in seconds from generating paths = ' + str(t.time() - start))\n",
        "    def lower_bound(self, K):    \n",
        "        X = geometricBM(K, n+1, d, T, S0, r, delta, sigma, pho)\n",
        "            \n",
        "        input_nn = tf.Variable(X, name = 'x', dtype = float_type)\n",
        "        s = []\n",
        "        for i in range(self.n - 1, -1, -1):\n",
        "            s.append( self.NN[i].run(input_nn))\n",
        "        s.append(tf.ones([K, 1], dtype = int_type)) \n",
        "        \n",
        "        tau = 0\n",
        "        for i in range(self.n + 1):\n",
        "            p = 1\n",
        "            for j in range(i):\n",
        "                p *= 1 - s[j]\n",
        "            tau += i * s[i] * p\n",
        "        \n",
        "        print(payoff(tf.squeeze(tau), input_nn))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVQd58eu9ewV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNet:\n",
        "    '''\n",
        "    This class will be responsable for each individual neural network\n",
        "    In our model, it will be the F^(theta_n)\n",
        "    '''\n",
        "    def __init__(self, n):\n",
        "        xavier=tf.keras.initializers.GlorotUniform()\n",
        "        self.l1=tf.keras.layers.Dense(42,kernel_initializer = xavier, activation=tf.nn.relu,input_shape=[batch_size,n,d], dtype = float_type)\n",
        "        self.l2=tf.keras.layers.Dense(42,kernel_initializer = xavier,activation=tf.nn.relu, dtype = float_type)\n",
        "        self.out=tf.keras.layers.Dense(1,kernel_initializer = xavier,activation = tf.nn.sigmoid, dtype = float_type)\n",
        "        self.train_op = tf.keras.optimizers.Adam(0.1)\n",
        "        self.n = n\n",
        "        self.stop_time = []\n",
        "        \n",
        "    # Running the model\n",
        "    def run(self,X): \n",
        "        boom=self.l1(X)\n",
        "        boom1=self.l2(boom)\n",
        "        boom2=self.out(boom1)\n",
        "        \n",
        "        boom2 = tf.where(tf.greater(boom2, 0.5), 1, 0)\n",
        "        return tf.cast(boom2[:,self.n], int_type)\n",
        "      \n",
        "    #Custom loss fucntion\n",
        "    #Change this for each n\n",
        "    def get_loss(self,X):\n",
        "        boom=self.l1(X)\n",
        "        boom1=self.l2(boom)\n",
        "        boom2=self.out(boom1)\n",
        "        #print(self.n)\n",
        "\n",
        "        #print(self.stop_time)\n",
        "        #print(payoff(self.stop_time,X))\n",
        "        r = payoff(self.n-1,X)*boom2 +  payoff(self.stop_time,X)*(1-boom2)\n",
        "        \n",
        "        return -r\n",
        "      \n",
        "    # get gradients\n",
        "    def get_grad(self,X):\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(self.l1.variables)\n",
        "            tape.watch(self.l2.variables)\n",
        "            tape.watch(self.out.variables)\n",
        "            L = self.get_loss(X)\n",
        "            g = tape.gradient(L, [self.l1.variables[0],self.l1.variables[1],self.l2.variables[0],self.l2.variables[1],self.out.variables[0],self.out.variables[1]])\n",
        "        return g\n",
        "      \n",
        "    # perform gradient descent\n",
        "    def network_learn(self,X, stop_time):\n",
        "        for i in range(training_size):    \n",
        "            self.stop_time = stop_time[i*batch_size:(i + 1)*batch_size]\n",
        "            g = self.get_grad(X[i*batch_size:(i + 1)*batch_size])\n",
        "            self.train_op.apply_gradients(zip(g, [self.l1.variables[0],self.l1.variables[1],self.l2.variables[0],self.l2.variables[1],self.out.variables[0],self.out.variables[1]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldKAnLJw_bsu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "820fdc6f-7f30-4ab3-b211-9b357c5d8afc"
      },
      "source": [
        "batch_size = 8192\n",
        "training_size = 100\n",
        "P = Pricer(n)\n",
        "for i in range(5):\n",
        "  \n",
        "  P.train()\n",
        "  print(str(i) + ' The price is: ')\n",
        "  P.lower_bound(100000)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Getting X\n",
            "Training layer i = \n",
            "8\n",
            "Training layer i = \n",
            "7\n",
            "Training layer i = \n",
            "6\n",
            "Training layer i = \n",
            "5\n",
            "Training layer i = \n",
            "4\n",
            "Training layer i = \n",
            "3\n",
            "Training layer i = \n",
            "2\n",
            "Training layer i = \n",
            "1\n",
            "Training layer i = \n",
            "0\n",
            "End of training, time in seconds from generating paths = 13.070754289627075\n",
            "0 The price is: \n",
            "tf.Tensor(21.412269373002882, shape=(), dtype=float64)\n",
            "Getting X\n",
            "Training layer i = \n",
            "8\n",
            "Training layer i = \n",
            "7\n",
            "Training layer i = \n",
            "6\n",
            "Training layer i = \n",
            "5\n",
            "Training layer i = \n",
            "4\n",
            "Training layer i = \n",
            "3\n",
            "Training layer i = \n",
            "2\n",
            "Training layer i = \n",
            "1\n",
            "Training layer i = \n",
            "0\n",
            "End of training, time in seconds from generating paths = 12.706957578659058\n",
            "1 The price is: \n",
            "tf.Tensor(21.390256989175576, shape=(), dtype=float64)\n",
            "Getting X\n",
            "Training layer i = \n",
            "8\n",
            "Training layer i = \n",
            "7\n",
            "Training layer i = \n",
            "6\n",
            "Training layer i = \n",
            "5\n",
            "Training layer i = \n",
            "4\n",
            "Training layer i = \n",
            "3\n",
            "Training layer i = \n",
            "2\n",
            "Training layer i = \n",
            "1\n",
            "Training layer i = \n",
            "0\n",
            "End of training, time in seconds from generating paths = 12.495606422424316\n",
            "2 The price is: \n",
            "tf.Tensor(21.369134601510094, shape=(), dtype=float64)\n",
            "Getting X\n",
            "Training layer i = \n",
            "8\n",
            "Training layer i = \n",
            "7\n",
            "Training layer i = \n",
            "6\n",
            "Training layer i = \n",
            "5\n",
            "Training layer i = \n",
            "4\n",
            "Training layer i = \n",
            "3\n",
            "Training layer i = \n",
            "2\n",
            "Training layer i = \n",
            "1\n",
            "Training layer i = \n",
            "0\n",
            "End of training, time in seconds from generating paths = 12.630319118499756\n",
            "3 The price is: \n",
            "tf.Tensor(21.25178131740381, shape=(), dtype=float64)\n",
            "Getting X\n",
            "Training layer i = \n",
            "8\n",
            "Training layer i = \n",
            "7\n",
            "Training layer i = \n",
            "6\n",
            "Training layer i = \n",
            "5\n",
            "Training layer i = \n",
            "4\n",
            "Training layer i = \n",
            "3\n",
            "Training layer i = \n",
            "2\n",
            "Training layer i = \n",
            "1\n",
            "Training layer i = \n",
            "0\n",
            "End of training, time in seconds from generating paths = 12.629092931747437\n",
            "4 The price is: \n",
            "tf.Tensor(21.434067915966317, shape=(), dtype=float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p9N0YXu_c4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGcXZoSXDk64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}